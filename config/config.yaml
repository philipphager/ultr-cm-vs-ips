defaults:
  - _self_
  - data: synthetic
  - baseline: lgbm
  - simulation: graded-pbm
  - model: ips
  - hyperparameter: "${model}_${data}"

experiment:
  name: default

random_state: 0

trainer:
  _target_: pytorch_lightning.Trainer
  max_epochs: 200
  accelerator: auto
  # val_check_interval: 10_000 # Evaluate every 500k clicks, use when not aggregating clicks
  logger:
    - _target_: src.logger.LocalLogger
  callbacks:
    -
     _target_: pytorch_lightning.callbacks.EarlyStopping
     monitor: val_loss
     patience: 5

train_loader:
  _target_: torch.utils.data.DataLoader
  dataset: ???
  batch_size: 50
  shuffle: True

val_test_loader:
  _target_: torch.utils.data.DataLoader
  dataset: ???
  batch_size: 500
  shuffle: False


hydra:
  job:
    chdir: True
  output_subdir: .
  run:
    dir: /ivi/ilps/personal/sgupta/philipp-project/ultr-cm-vs-ips/results/${experiment.name}
  sweep:
    dir: /ivi/ilps/personal/sgupta/philipp-project/ultr-cm-vs-ips/results/${experiment.name}
    subdir: ${hydra.job.override_dirname}
  launcher:
    mem_gb: 32
    cpus_per_task: 4
    array_parallelism: 2
    partition: gpu
    gres: gpu:1
