{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dc0b1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philipphager/.pyenv/versions/3.9.7/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a58f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy(nn.Module):\n",
    "    def forward(\n",
    "        self,\n",
    "        y_predict: torch.Tensor,\n",
    "        y_true: torch.Tensor,\n",
    "        eps: float = 1e-10,\n",
    "    ) -> torch.float:\n",
    "\n",
    "        loss = -(\n",
    "            (y_true) * torch.log(y_predict.clip(min=eps))\n",
    "            + (1 - y_true) * torch.log((1 - y_predict).clip(min=eps))\n",
    "        )\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "664d3bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CM(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(5, 1)\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.linear(x)).squeeze() * self.p\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa280d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from pytorchltr.evaluation import arp, ndcg\n",
    "\n",
    "\n",
    "def get_metrics(\n",
    "    y_predict: torch.Tensor, y_true: torch.Tensor, n: torch.Tensor, prefix: str = \"\"\n",
    "):\n",
    "    return {\n",
    "        f\"{prefix}arp\": arp(y_predict, y_true, n).mean().detach(),\n",
    "        f\"{prefix}ndcg@1\": ndcg(y_predict, y_true, n, k=1).mean().detach(),\n",
    "        f\"{prefix}ndcg@5\": ndcg(y_predict, y_true, n, k=5).mean().detach(),\n",
    "        f\"{prefix}ndcg@10\": ndcg(y_predict, y_true, n, k=10).mean().detach(),\n",
    "        f\"{prefix}ndcg\": ndcg(y_predict, y_true, n).mean().detach(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a9e86831",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arp': tensor(29.2660), 'ndcg@1': tensor(1.), 'ndcg@5': tensor(1.), 'ndcg@10': tensor(1.), 'ndcg': tensor(1.)}\n",
      "{'arp': tensor(34.3368), 'ndcg@1': tensor(0.4340), 'ndcg@5': tensor(0.4340), 'ndcg@10': tensor(0.4340), 'ndcg': tensor(0.7128)}\n",
      "{'arp': tensor(36.8582), 'ndcg@1': tensor(1.), 'ndcg@5': tensor(1.), 'ndcg@10': tensor(1.), 'ndcg': tensor(0.9326)}\n",
      "{'arp': tensor(39.4568), 'ndcg@1': tensor(0.4340), 'ndcg@5': tensor(0.4340), 'ndcg@10': tensor(0.4340), 'ndcg': tensor(0.6326)}\n",
      "{'arp': tensor(42.3736), 'ndcg@1': tensor(0.2142), 'ndcg@5': tensor(0.2142), 'ndcg@10': tensor(0.2142), 'ndcg': tensor(0.5186)}\n",
      "{'arp': tensor(36.4034), 'ndcg@1': tensor(1.), 'ndcg@5': tensor(1.), 'ndcg@10': tensor(1.), 'ndcg': tensor(0.9313)}\n",
      "{'arp': tensor(36.4034), 'ndcg@1': tensor(1.), 'ndcg@5': tensor(1.), 'ndcg@10': tensor(1.), 'ndcg': tensor(0.9313)}\n",
      "{'arp': tensor(50.6860), 'ndcg@1': tensor(0.0718), 'ndcg@5': tensor(0.0718), 'ndcg@10': tensor(0.0718), 'ndcg': tensor(0.3864)}\n",
      "{'arp': tensor(54.9894), 'ndcg@1': tensor(0.2142), 'ndcg@5': tensor(0.2142), 'ndcg@10': tensor(0.2142), 'ndcg': tensor(0.4090)}\n",
      "{'arp': tensor(36.2704), 'ndcg@1': tensor(1.), 'ndcg@5': tensor(1.), 'ndcg@10': tensor(1.), 'ndcg': tensor(0.9358)}\n"
     ]
    }
   ],
   "source": [
    "n_results = 100\n",
    "y = torch.randint(5, (3, n_results))\n",
    "x = F.one_hot(y).float()\n",
    "rows = []\n",
    "\n",
    "epochs = 100\n",
    "lr = 0.1\n",
    "y = 0.1 + 0.9 * (2 ** y - 1) / (2 ** 4 - 1)\n",
    "\n",
    "for eta in range(10):\n",
    "    grad = []    \n",
    "   \n",
    "    #p = (1 / (1 + torch.arange(n_results))) ** eta\n",
    "    p = 1 / (1 + eta)\n",
    "    c = y * p\n",
    "\n",
    "    model = CM(p)\n",
    "    loss = BinaryCrossEntropy()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        y_predict = model(x)\n",
    "\n",
    "        l = loss(y_predict, c)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        grad.append(model.linear.weight.grad.squeeze())\n",
    "        \n",
    "    rows.append({\n",
    "        \"eta\": eta,\n",
    "        \"grad_mean\": float(torch.stack(grad).abs().mean()),\n",
    "        \"grad_std\": float(torch.stack(grad).abs().std())\n",
    "    })\n",
    "    y_predict = torch.sigmoid(model.linear(x))\n",
    "    print(get_metrics(y_predict, y, torch.full((3,), n_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "610450b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CM(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Sequential(nn.Linear(2, 1))\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.linear(x)).squeeze() * self.p\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "601fea5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4.0771e-04, 6.0810e-01], grad_fn=<PowBackward0>),\n",
       " tensor([0.1202, 0.1202], grad_fn=<SqueezeBackward0>))"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_results = 100\n",
    "y = torch.randint(5, (3, n_results))\n",
    "x = F.one_hot(y).float()\n",
    "rows = []\n",
    "\n",
    "epochs = 10\n",
    "lr = 0.1\n",
    "y = 0.1 + 0.9 * (2 ** y - 1) / (2 ** 4 - 1)\n",
    "\n",
    "x = torch.tensor([\n",
    "    [1, 0],\n",
    "    [1, 0]\n",
    "]).float()\n",
    "\n",
    "y = torch.tensor([\n",
    "    0.1, 0.9,\n",
    "])\n",
    "\n",
    "p = torch.tensor([\n",
    "    [1.0, 0.0001],\n",
    "])\n",
    "\n",
    "\n",
    "c = y * p\n",
    "\n",
    "model = CM(p)\n",
    "loss = BinaryCrossEntropy()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_predict = model(x)\n",
    "\n",
    "    l = loss(y_predict, c)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "y_predict = torch.sigmoid(model.linear(x)).squeeze()\n",
    "(y - y_predict) ** 2, y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ab11b68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropyWithLogits(nn.Module):\n",
    "    def forward(\n",
    "        self,\n",
    "        y_predict: torch.Tensor,\n",
    "        y_true: torch.Tensor,\n",
    "        position_bias: torch.Tensor,\n",
    "        eps: float = 1e-10,\n",
    "    ) -> torch.float:\n",
    "        \"\"\"\n",
    "        Binary Cross-Entropy with IPS as in Bekker2019, Saito2020, Oosterhuis2022\n",
    "        https://arxiv.org/pdf/2203.17118.pdf\n",
    "        https://arxiv.org/pdf/1909.03601.pdf\n",
    "\n",
    "        Args:\n",
    "            y_predict: Tensor of size (n_batch, n_results) with predicted relevance\n",
    "            y_true: Tensor of size (n_batch, n_results) with ground_truth scores\n",
    "            position_bias: Tensor of size (n_results) with propensities per rank\n",
    "            clip: Min propensity used to clip position_bias\n",
    "            eps: Min value to avoid ln(0) = -inf\n",
    "\n",
    "        Returns:\n",
    "            Mean aggregated loss for the given batch\n",
    "        \"\"\"\n",
    "        y_predict = torch.sigmoid(y_predict)\n",
    "        position_bias = position_bias.type_as(y_predict)\n",
    "\n",
    "        loss = -(\n",
    "            (y_true / position_bias) * torch.log(y_predict.clip(min=eps))\n",
    "            + (1 - (y_true / position_bias)) * torch.log((1 - y_predict).clip(min=eps))\n",
    "        )\n",
    "\n",
    "        return loss.sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e16e826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = nn.ELU()(x).squeeze()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "416898d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1605, 0.1595], grad_fn=<PowBackward0>),\n",
       " tensor([0.5007, 0.5007], grad_fn=<SqueezeBackward0>))"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_results = 100\n",
    "y = torch.randint(5, (3, n_results))\n",
    "x = F.one_hot(y).float()\n",
    "rows = []\n",
    "\n",
    "epochs = 100\n",
    "lr = 0.1\n",
    "y = 0.1 + 0.9 * (2 ** y - 1) / (2 ** 4 - 1)\n",
    "\n",
    "x = torch.tensor([\n",
    "    [1, 0],\n",
    "    [1, 0]\n",
    "]).float()\n",
    "\n",
    "y = torch.tensor([\n",
    "    0.1, 0.9,\n",
    "])\n",
    "\n",
    "p = torch.tensor([\n",
    "    [1.0, 0.0001],\n",
    "])\n",
    "\n",
    "c = y * p\n",
    "\n",
    "model = IPS()\n",
    "loss = BinaryCrossEntropyWithLogits()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_predict = model(x)\n",
    "\n",
    "    l = loss(y_predict, c, p)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "y_predict = torch.sigmoid(model.linear(x)).squeeze()\n",
    "(y - y_predict) ** 2, y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "139b8351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-1ba7250d819747da8bbf67600a2a706c\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-1ba7250d819747da8bbf67600a2a706c\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-1ba7250d819747da8bbf67600a2a706c\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-b2b3a253afd6178dd57b8c25391a3e1e\"}, \"mark\": \"line\", \"encoding\": {\"x\": {\"field\": \"eta\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"grad_mean\", \"type\": \"quantitative\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-b2b3a253afd6178dd57b8c25391a3e1e\": [{\"eta\": 0, \"grad_mean\": 0.03331398218870163, \"grad_std\": 0.024764029309153557}, {\"eta\": 1, \"grad_mean\": 0.01540885679423809, \"grad_std\": 0.007680076640099287}, {\"eta\": 2, \"grad_mean\": 0.011078014969825745, \"grad_std\": 0.007957064546644688}, {\"eta\": 3, \"grad_mean\": 0.008961096405982971, \"grad_std\": 0.004401240032166243}, {\"eta\": 4, \"grad_mean\": 0.006093140691518784, \"grad_std\": 0.004082388710230589}, {\"eta\": 5, \"grad_mean\": 0.005274083930999041, \"grad_std\": 0.0024660013150423765}, {\"eta\": 6, \"grad_mean\": 0.004363420885056257, \"grad_std\": 0.002090784488245845}, {\"eta\": 7, \"grad_mean\": 0.003722009714692831, \"grad_std\": 0.0020279488526284695}, {\"eta\": 8, \"grad_mean\": 0.003273693611845374, \"grad_std\": 0.0015881967265158892}, {\"eta\": 9, \"grad_mean\": 0.003141154069453478, \"grad_std\": 0.0025660288520157337}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(pd.DataFrame(rows)).mark_line().encode(\n",
    "    x=\"eta\",\n",
    "    y=\"grad_mean\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
